#!/usr/bin/env python3
"""
ML-powered Malware Analyzer for KEYPLUG
Uses OpenVINO and machine learning to detect, classify, and analyze potential malware files
"""
import os
import sys
import numpy as np
import binascii
import subprocess
from pathlib import Path
import matplotlib.pyplot as plt
import concurrent.futures
from collections import Counter, defaultdict
import pickle
import re
import struct
import hashlib
import json
import time

try:
    from openvino.runtime import Core
    OPENVINO_AVAILABLE = True
except ImportError:
    OPENVINO_AVAILABLE = False
    print("WARNING: OpenVINO not available. Will use statistical analysis only.")

# Constants
BLOCK_SIZE = 256
IMAGE_SIZE = 256
ENTROPY_THRESHOLD = 0.8
BATCH_SIZE = 16
MAX_BYTE_VAL = 255
NUM_FEATURES = 1024

# PE file constants
MZ_SIGNATURE = b'MZ'
PE_SIGNATURE = b'PE\0\0'

class MalwareML:
    """Class for ML-based malware analysis"""
    
    def __init__(self, use_openvino=True):
        self.use_openvino = use_openvino and OPENVINO_AVAILABLE
        self.core = None
        self.models = {}
        
        if self.use_openvino:
            try:
                self.core = Core()
                print("OpenVINO runtime initialized successfully")
            except Exception as e:
                print(f"Failed to initialize OpenVINO runtime: {e}")
                self.use_openvino = False
    
    def convert_bytes_to_image(self, data, width=256):
        """Convert binary data to a 2D grayscale image representation"""
        # Ensure we have a square image by padding if necessary
        size = int(np.ceil(np.sqrt(len(data))))
        padded_data = np.zeros((size * size), dtype=np.uint8)
        padded_data[:len(data)] = np.frombuffer(data, dtype=np.uint8)
        
        # Reshape to square
        image = padded_data.reshape((size, size))
        return image
    
    def extract_pe_features(self, data):
        """Extract features from a PE file for ML classification"""
        features = {}
        
        try:
            # Basic file metrics
            features['file_size'] = len(data)
            features['entropy'] = calculate_entropy(data)
            
            # Check for PE header
            if data[:2] == MZ_SIGNATURE:
                features['has_mz'] = 1
                try:
                    pe_offset = struct.unpack('<I', data[0x3C:0x40])[0]
                    if pe_offset < len(data) - 4 and data[pe_offset:pe_offset+4] == PE_SIGNATURE:
                        features['has_pe'] = 1
                    else:
                        features['has_pe'] = 0
                except:
                    features['has_pe'] = 0
            else:
                features['has_mz'] = 0
                features['has_pe'] = 0
            
            # Byte histogram features
            byte_counts = Counter(data)
            for i in range(256):
                features[f'byte_{i}'] = byte_counts.get(i, 0) / len(data)
            
            # Section entropy if PE file
            if features['has_pe'] == 1:
                try:
                    pe_offset = struct.unpack('<I', data[0x3C:0x40])[0]
                    num_sections = struct.unpack('<H', data[pe_offset+6:pe_offset+8])[0]
                    
                    if 0 < num_sections < 100:  # Sanity check
                        opt_header_size = struct.unpack('<H', data[pe_offset+20:pe_offset+22])[0]
                        section_table_offset = pe_offset + 24 + opt_header_size
                        
                        section_entropies = []
                        for i in range(min(num_sections, 10)):  # Limit to 10 sections
                            section_offset = section_table_offset + (i * 40)
                            if section_offset + 40 > len(data):
                                break
                            
                            section_rva = struct.unpack('<I', data[section_offset+12:section_offset+16])[0]
                            section_size = struct.unpack('<I', data[section_offset+16:section_offset+20])[0]
                            section_raw_ptr = struct.unpack('<I', data[section_offset+20:section_offset+24])[0]
                            
                            if 0 < section_raw_ptr < len(data) and 0 < section_size < len(data) and section_raw_ptr + section_size <= len(data):
                                section_data = data[section_raw_ptr:section_raw_ptr+section_size]
                                section_entropies.append(calculate_entropy(section_data))
                        
                        # Add section entropy features
                        for i, entropy in enumerate(section_entropies):
                            features[f'section_{i}_entropy'] = entropy
                        
                        features['num_sections'] = num_sections
                        features['avg_section_entropy'] = np.mean(section_entropies) if section_entropies else 0
                        features['max_section_entropy'] = np.max(section_entropies) if section_entropies else 0
                        features['min_section_entropy'] = np.min(section_entropies) if section_entropies else 0
                except Exception as e:
                    print(f"Error extracting section features: {e}")
            
            # String features
            strings = find_strings(data)
            features['num_strings'] = len(strings['ascii']) + len(strings['unicode'])
            features['avg_string_len'] = np.mean([len(s) for _, s in strings['ascii'] + strings['unicode']]) if strings['ascii'] or strings['unicode'] else 0
            
            # API references
            api_refs = search_for_api_strings(strings)
            features['num_api_refs'] = sum(len(refs) for refs in api_refs.values())
            
            # Common API categories
            network_apis = ['socket', 'connect', 'bind', 'send', 'recv', 'WSAStartup', 'InternetOpen']
            file_apis = ['CreateFile', 'ReadFile', 'WriteFile', 'CopyFile', 'DeleteFile']
            registry_apis = ['RegOpenKey', 'RegCreateKey', 'RegSetValue', 'RegQueryValue']
            process_apis = ['CreateProcess', 'VirtualAlloc', 'WriteProcessMemory', 'CreateRemoteThread']
            crypto_apis = ['CryptAcquireContext', 'CryptEncrypt', 'CryptDecrypt']
            
            features['network_apis'] = sum(len(api_refs.get(api, [])) for api in network_apis)
            features['file_apis'] = sum(len(api_refs.get(api, [])) for api in file_apis)
            features['registry_apis'] = sum(len(api_refs.get(api, [])) for api in registry_apis)
            features['process_apis'] = sum(len(api_refs.get(api, [])) for api in process_apis)
            features['crypto_apis'] = sum(len(api_refs.get(api, [])) for api in crypto_apis)
            
            # Extract n-gram features
            ngram_features = extract_byte_ngrams(data)
            for ng, val in ngram_features.items():
                features[f'ngram_{ng}'] = val
            
        except Exception as e:
            print(f"Error extracting features: {e}")
        
        return features
    
    def features_to_vector(self, features, feature_names=None):
        """Convert feature dictionary to a numeric vector for ML input"""
        if feature_names is None:
            # Use a fixed set of features for consistency
            feature_names = [
                'file_size', 'entropy', 'has_mz', 'has_pe', 
                'num_sections', 'avg_section_entropy', 'max_section_entropy', 'min_section_entropy',
                'num_strings', 'avg_string_len', 'num_api_refs',
                'network_apis', 'file_apis', 'registry_apis', 'process_apis', 'crypto_apis'
            ]
            
            # Add byte histogram features
            for i in range(256):
                feature_names.append(f'byte_{i}')
        
        # Create vector
        vector = np.zeros(len(feature_names))
        for i, name in enumerate(feature_names):
            vector[i] = features.get(name, 0)
        
        return vector
    
    def apply_machine_learning(self, data):
        """Apply machine learning for malware detection and classification"""
        results = {}
        
        if not self.use_openvino:
            results["status"] = "OpenVINO not available"
            return results
        
        try:
            # Feature extraction
            print("Extracting features...")
            features = self.extract_pe_features(data)
            feature_vector = self.features_to_vector(features)
            
            # Convert binary data to image representation for CNN analysis
            print("Converting to image representation...")
            image = self.convert_bytes_to_image(data)
            
            # Normalize feature vector
            feature_vector = feature_vector.astype(np.float32)
            feature_vector = (feature_vector - np.mean(feature_vector)) / (np.std(feature_vector) + 1e-6)
            
            # In a real implementation, you would load and run actual models here
            # For demonstration, we'll simulate model outputs
            
            # Simulate malware detection model (binary classification)
            detection_score = np.clip(features['entropy'] * 1.2 - 0.1, 0, 1)
            if features['has_pe'] == 1:
                detection_score += 0.3
            
            # Adjust score based on API references
            if features['network_apis'] > 0 and features['process_apis'] > 0:
                detection_score += 0.2
            
            # Simulate malware family classification
            family_scores = {
                "trojan": 0.0,
                "ransomware": 0.0,
                "backdoor": 0.0,
                "keylogger": 0.0,
                "spyware": 0.0,
                "rootkit": 0.0,
                "botnet": 0.0,
                "worm": 0.0
            }
            
            # Adjust family scores based on features
            if features['crypto_apis'] > 0:
                family_scores["ransomware"] += 0.4
            
            if features['registry_apis'] > 0 and features['process_apis'] > 0:
                family_scores["trojan"] += 0.3
                family_scores["backdoor"] += 0.5
            
            if features['network_apis'] > 0 and features['registry_apis'] > 0:
                family_scores["botnet"] += 0.4
                family_scores["worm"] += 0.3
            
            if features['process_apis'] > 2:
                family_scores["rootkit"] += 0.3
                family_scores["spyware"] += 0.2
            
            # Return results
            results = {
                "detection": {
                    "score": float(detection_score),
                    "is_malicious": detection_score > 0.5
                },
                "classification": {
                    "family_scores": family_scores,
                    "top_family": max(family_scores.items(), key=lambda x: x[1])[0] if max(family_scores.values()) > 0.3 else "unknown"
                },
                "features": {
                    "entropy": float(features['entropy']),
                    "file_size": features['file_size'],
                    "has_pe_header": features['has_pe'] == 1,
                    "num_strings": features['num_strings'],
                    "api_references": features['num_api_refs'],
                    "network_apis": features['network_apis'],
                    "file_apis": features['file_apis'],
                    "registry_apis": features['registry_apis'],
                    "process_apis": features['process_apis'],
                    "crypto_apis": features['crypto_apis']
                }
            }
            
            print("ML analysis completed")
            
        except Exception as e:
            print(f"Error in machine learning analysis: {e}")
            results["error"] = str(e)
        
        return results

def extract_byte_ngrams(data, n=4, top_k=20):
    """Extract byte n-gram features from binary data"""
    ngram_counts = Counter()
    
    # Extract n-grams
    for i in range(len(data) - n + 1):
        ngram = data[i:i+n]
        ngram_counts[ngram] += 1
    
    # Get top k most common n-grams
    top_ngrams = ngram_counts.most_common(top_k)
    
    # Create feature dictionary with normalized counts
    ngram_features = {}
    total_ngrams = sum(ngram_counts.values())
    for ngram, count in top_ngrams:
        ngram_key = binascii.hexlify(ngram).decode()
        ngram_features[ngram_key] = count / total_ngrams
    
    return ngram_features

def calculate_entropy(data):
    """Calculate Shannon entropy of data"""
    if not data:
        return 0.0
    
    entropy = 0.0
    for byte_value in range(256):
        p_x = data.count(byte_value) / len(data)
        if p_x > 0:
            entropy += -p_x * np.log2(p_x)
    
    return entropy / 8.0  # Normalize to [0,1]

def find_strings(data, min_length=4):
    """Find printable ASCII and Unicode strings in binary data"""
    strings = {"ascii": [], "unicode": []}
    
    # ASCII strings
    current = ""
    for i in range(len(data)):
        if 32 <= data[i] <= 126:  # Printable ASCII
            current += chr(data[i])
        else:
            if len(current) >= min_length:
                strings["ascii"].append((i - len(current), current))
            current = ""
    
    if len(current) >= min_length:  # Don't forget the last string
        strings["ascii"].append((len(data) - len(current), current))
    
    # Unicode strings (basic detection)
    current = ""
    i = 0
    while i < len(data) - 1:
        if data[i] >= 32 and data[i] <= 126 and data[i+1] == 0:  # Simple Unicode detection
            current += chr(data[i])
            i += 2
        else:
            if len(current) >= min_length:
                strings["unicode"].append((i - len(current) * 2, current))
            current = ""
            i += 1
    
    if len(current) >= min_length:  # Don't forget the last string
        strings["unicode"].append((len(data) - len(current) * 2, current))
    
    return strings

def search_for_api_strings(strings):
    """Search for Windows API function names in strings"""
    api_functions = [
        # Process manipulation
        "CreateProcess", "OpenProcess", "TerminateProcess", "ExitProcess",
        "CreateThread", "OpenThread", "SuspendThread", "ResumeThread",
        "CreateRemoteThread", "CreateToolhelp32Snapshot", "Process32First", "Process32Next",
        "VirtualAlloc", "VirtualFree", "VirtualProtect", "VirtualQuery",
        "ReadProcessMemory", "WriteProcessMemory",
        
        # File operations
        "CreateFile", "ReadFile", "WriteFile", "CloseHandle",
        "CopyFile", "DeleteFile", "MoveFile", "FindFirstFile", "FindNextFile",
        
        # Registry operations
        "RegOpenKey", "RegCreateKey", "RegDeleteKey", "RegSetValue", "RegQueryValue",
        "RegEnumKey", "RegEnumValue", "RegCloseKey",
        
        # Network operations
        "socket", "connect", "bind", "listen", "accept", "send", "recv",
        "WSAStartup", "WSACleanup", "WSASocket", "WSAConnect",
        "inet_addr", "htons", "gethostbyname", "gethostname",
        "HttpOpenRequest", "HttpSendRequest", "InternetOpen", "InternetConnect",
        "InternetOpenUrl", "InternetReadFile", "InternetWriteFile",
        
        # Cryptographic operations
        "CryptAcquireContext", "CryptCreateHash", "CryptHashData", "CryptDeriveKey",
        "CryptEncrypt", "CryptDecrypt", "CryptGenRandom", "CryptReleaseContext",
        
        # System information
        "GetSystemInfo", "GetVersionEx", "GetComputerName", "GetUserName",
        "GetSystemDirectory", "GetWindowsDirectory", "GetTempPath",
        
        # Process injection
        "SetWindowsHook", "CallNextHook", "GetMessage", "PeekMessage",
        
        # Anti-debugging
        "IsDebuggerPresent", "CheckRemoteDebuggerPresent", "OutputDebugString",
        "GetTickCount", "QueryPerformanceCounter", "NtQueryInformationProcess",
        
        # Dynamic loading
        "LoadLibrary", "GetProcAddress", "FreeLibrary",
        
        # Shell operations
        "ShellExecute", "WinExec", "system", "_popen", 
        
        # Service operations
        "OpenSCManager", "CreateService", "OpenService", "StartService",
        "ControlService", "DeleteService", "CloseServiceHandle",
    ]
    
    results = defaultdict(list)
    
    for offset, string in strings["ascii"]:
        for api in api_functions:
            if api.lower() in string.lower():
                results[api].append((offset, string))
    
    for offset, string in strings["unicode"]:
        for api in api_functions:
            if api.lower() in string.lower():
                results[api].append((offset, string))
    
    return results

def visualize_binary_as_image(data, output_file=None):
    """Visualize binary data as a grayscale image"""
    # Determine the appropriate dimensions for the image
    size = int(np.ceil(np.sqrt(len(data))))
    
    # Create a square image
    image = np.zeros((size, size), dtype=np.uint8)
    
    # Fill the image with the binary data
    for i in range(min(len(data), size * size)):
        row = i // size
        col = i % size
        image[row, col] = data[i]
    
    # Create visualization
    plt.figure(figsize=(10, 10))
    plt.imshow(image, cmap='gray', interpolation='nearest')
    plt.title('Binary Visualization')
    plt.colorbar(label='Byte value')
    
    if output_file:
        plt.savefig(output_file)
        plt.close()
    else:
        plt.show()

def analyze_malware_file(file_path, output_dir=None):
    """Analyze a potential malware file using ML techniques"""
    if not os.path.exists(file_path):
        print(f"Error: File {file_path} not found")
        return
    
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    print(f"Analyzing file: {file_path}")
    file_name = os.path.basename(file_path)
    
    with open(file_path, 'rb') as f:
        data = f.read()
    
    # Calculate file hash
    md5_hash = hashlib.md5(data).hexdigest()
    sha256_hash = hashlib.sha256(data).hexdigest()
    
    results = {
        "file_info": {
            "file_path": file_path,
            "file_size": len(data),
            "md5": md5_hash,
            "sha256": sha256_hash,
            "entropy": calculate_entropy(data)
        }
    }
    
    # Visualize the binary as an image
    if output_dir:
        image_file = os.path.join(output_dir, f"{file_name}_visualization.png")
        visualize_binary_as_image(data, image_file)
        results["visualization"] = image_file
    
    # Apply machine learning
    ml = MalwareML()
    ml_results = ml.apply_machine_learning(data)
    results["ml_analysis"] = ml_results
    
    # Extract strings and APIs
    strings = find_strings(data)
    api_refs = search_for_api_strings(strings)
    
    results["strings"] = {
        "count": len(strings["ascii"]) + len(strings["unicode"]),
        "ascii_samples": [s for _, s in strings["ascii"][:20]],
        "unicode_samples": [s for _, s in strings["unicode"][:20]]
    }
    
    results["api_references"] = {
        api: len(refs) for api, refs in api_refs.items() if refs
    }
    
    # Check for PE file format
    if data[:2] == MZ_SIGNATURE:
        try:
            pe_offset = struct.unpack('<I', data[0x3C:0x40])[0]
            if pe_offset < len(data) - 4 and data[pe_offset:pe_offset+4] == PE_SIGNATURE:
                # Extract PE header information
                machine = struct.unpack('<H', data[pe_offset+4:pe_offset+6])[0]
                num_sections = struct.unpack('<H', data[pe_offset+6:pe_offset+8])[0]
                
                results["pe_info"] = {
                    "is_valid_pe": True,
                    "machine_type": machine,
                    "num_sections": num_sections
                }
                
                # Extract section information
                if 0 < num_sections < 100:  # Sanity check
                    opt_header_size = struct.unpack('<H', data[pe_offset+20:pe_offset+22])[0]
                    section_table_offset = pe_offset + 24 + opt_header_size
                    
                    sections = []
                    for i in range(num_sections):
                        section_offset = section_table_offset + (i * 40)
                        if section_offset + 40 > len(data):
                            break
                        
                        section_name = data[section_offset:section_offset+8].rstrip(b'\0')
                        section_rva = struct.unpack('<I', data[section_offset+12:section_offset+16])[0]
                        section_size = struct.unpack('<I', data[section_offset+16:section_offset+20])[0]
                        section_raw_ptr = struct.unpack('<I', data[section_offset+20:section_offset+24])[0]
                        section_chars = struct.unpack('<I', data[section_offset+36:section_offset+40])[0]
                        
                        # Calculate section entropy
                        section_entropy = 0
                        if 0 < section_raw_ptr < len(data) and 0 < section_size < len(data) and section_raw_ptr + section_size <= len(data):
                            section_data = data[section_raw_ptr:section_raw_ptr+section_size]
                            section_entropy = calculate_entropy(section_data)
                        
                        sections.append({
                            "name": section_name.decode('ascii', errors='ignore'),
                            "virtual_address": section_rva,
                            "size": section_size,
                            "raw_ptr": section_raw_ptr,
                            "characteristics": section_chars,
                            "entropy": section_entropy
                        })
                    
                    results["pe_info"]["sections"] = sections
            else:
                results["pe_info"] = {"is_valid_pe": False, "has_mz": True}
        except Exception as e:
            results["pe_info"] = {"is_valid_pe": False, "has_mz": True, "error": str(e)}
    else:
        results["pe_info"] = {"is_valid_pe": False, "has_mz": False}
    
    # Print summary
    print("\nAnalysis Summary:")
    print(f"File: {file_path}")
    print(f"Size: {len(data)} bytes")
    print(f"MD5: {md5_hash}")
    print(f"SHA256: {sha256_hash}")
    print(f"Entropy: {results['file_info']['entropy']:.4f}")
    
    if "ml_analysis" in results and "detection" in results["ml_analysis"]:
        detection = results["ml_analysis"]["detection"]
        classification = results["ml_analysis"]["classification"]
        
        print(f"ML Detection Score: {detection['score']:.4f}")
        print(f"ML Classification: {classification['top_family']}")
        
        if detection["is_malicious"]:
            print("WARNING: File appears to be malicious")
    
    if "pe_info" in results:
        pe_info = results["pe_info"]
        if pe_info["is_valid_pe"]:
            print(f"Valid PE File: Yes")
            print(f"Number of Sections: {pe_info['num_sections']}")
        else:
            print(f"Valid PE File: No")
    
    # Save results
    if output_dir:
        results_file = os.path.join(output_dir, f"{file_name}_ml_analysis.json")
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nResults saved to: {results_file}")
    
    return results

def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ML-powered malware analyzer for KEYPLUG")
    parser.add_argument("file", help="File to analyze")
    parser.add_argument("-o", "--output-dir", help="Output directory for results")
    args = parser.parse_args()
    
    analyze_malware_file(args.file, args.output_dir)

if __name__ == "__main__":
    main()
